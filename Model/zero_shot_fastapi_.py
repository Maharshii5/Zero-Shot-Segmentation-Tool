# -*- coding: utf-8 -*-
"""Zero-Shot-FastAPI .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14PIs7zI234QiQc6trKRxYAs-tUPae3xP
"""

# This is a simple FastAPI API for the Zero-Shot Auditor.
# It's a high-performance alternative to the Flask version.

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import base64
from io import BytesIO
import cv2
import numpy as np
from PIL import Image
import torch
import torchvision.transforms as T
from groundingdino.util.inference import load_model, predict
from segment_anything import build_sam_vit_h, SamPredictor
import os

# Define the request body using Pydantic for automatic data validation
class SegmentationRequest(BaseModel):
    image: str
    prompt: str

app = FastAPI()

# --- 1. MODEL INITIALIZATION ---
# Load the models once when the FastAPI application starts.

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

def initialize_models():
    """Initializes and loads the Grounding DINO and SAM models."""
    print("Initializing Grounding DINO model...")
    dino_model = load_model("GroundingDINO_SwinT_OGC.py", "groundingdino_swint_ogc.pth")
    dino_model.to(DEVICE)
    dino_model.eval()

    print("Initializing SAM model...")
    sam = build_sam_vit_h(checkpoint="sam_vit_h_4b8939.pth")
    sam.to(DEVICE)
    sam_predictor = SamPredictor(sam)

    return dino_model, sam_predictor

# Load models globally
dino_model, sam_predictor = initialize_models()

# --- 2. HYBRID INFERENCE PIPELINE ---

def zero_shot_segmentation(image_bytes, text_prompt, dino_model, sam_predictor):
    """
    Performs zero-shot segmentation using Grounding DINO for bounding boxes
    and SAM for segmentation masks.
    """
    image_pil = Image.open(BytesIO(image_bytes)).convert("RGB")
    image_cv = np.array(image_pil)

    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    image_tensor = transform(image_pil).to(DEVICE)

    sam_predictor.set_image(image_cv)
    print(f"Running Grounding DINO with prompt: '{text_prompt}'...")
    dino_boxes, _, _ = predict(
        model=dino_model,
        image=image_tensor,
        caption=text_prompt,
        box_threshold=0.3,
        text_threshold=0.25
    )

    if len(dino_boxes) == 0:
        print("Grounding DINO did not detect any objects for the given prompt.")
        return None

    h, w, _ = image_cv.shape
    dino_boxes_numpy = dino_boxes.cpu().numpy()
    dino_boxes_xyxy = dino_boxes_numpy * np.array([w, h, w, h])

    print(f"Grounding DINO detected {len(dino_boxes)} potential objects. Now using SAM for segmentation...")
    masks, _, _ = sam_predictor.predict(
        point_coords=None,
        point_labels=None,
        box=dino_boxes_xyxy,
        multimask_output=False
    )

    return masks

# --- 3. VISUALIZATION ---

def visualize_results(image_bytes, masks):
    """
    Overlays the segmentation masks on the original image and returns a base64 string.
    """
    image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)
    colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0]]

    if masks is not None and len(masks) > 0:
        overlay = np.zeros_like(image, dtype=np.uint8)
        for i, mask in enumerate(masks):
            color = colors[i % len(colors)]
            overlay[mask] = color

        alpha = 0.5
        final_image = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)
    else:
        final_image = image

    _, buffer = cv2.imencode('.png', final_image)
    b64_image = base64.b64encode(buffer).decode('utf-8')
    return b64_image

# --- 4. API ENDPOINT ---

@app.post("/segment")
async def segment_image(data: SegmentationRequest):
    try:
        image_bytes = base64.b64decode(data.image)
        masks = zero_shot_segmentation(image_bytes, data.prompt, dino_model, sam_predictor)
        output_image_b64 = visualize_results(image_bytes, masks)

        return {
            "status": "success",
            "message": f"Found {len(masks) if masks is not None else 0} objects.",
            "image": output_image_b64
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# To run this, you would save it as a file (e.g., `main.py`) and use Uvicorn:
# uvicorn main:app --reload --host 0.0.0.0 --port 8000