# -*- coding: utf-8 -*-
"""Working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jsUdYaNYgnldYuNA66B78zbpmyMp_p7O
"""

!pip install -q groundingdino-py opencv-python Pillow
!pip install -q segment-anything
!pip install -q requests


!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
!wget -q https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py
!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import cv2
import numpy as np
from PIL import Image
import torch
import torchvision.transforms as T
from groundingdino.util.inference import load_model, predict
from segment_anything import build_sam_vit_h, SamPredictor
from google.colab.patches import cv2_imshow


def initialize_models():
    """Loads Grounding DINO and SAM models."""
    print("Initializing Grounding DINO model...")
    dino_model = load_model("GroundingDINO_SwinT_OGC.py", "groundingdino_swint_ogc.pth")
    print("Initializing SAM model...")
    sam = build_sam_vit_h(checkpoint="sam_vit_h_4b8939.pth")
    sam_predictor = SamPredictor(sam)
    return dino_model, sam_predictor



def run_segmentation(image_url, text_prompt, dino_model, sam_predictor):
    """
    Performs zero-shot segmentation on a hardcoded image URL.
    This function is a basic implementation without file uploading.
    """
    print(f"Loading image from {image_url}...")

    response = requests.get(image_url)
    image_pil = Image.open(BytesIO(response.content)).convert("RGB")
    image_cv = np.array(image_pil)

    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    image_tensor = transform(image_pil)

    sam_predictor.set_image(image_cv)

    print(f"Running Grounding DINO with prompt: '{text_prompt}'...")
    dino_boxes, _, _ = predict(
        model=dino_model,
        image=image_tensor,
        caption=text_prompt,
        box_threshold=0.3,
        text_threshold=0.25
    )

    if len(dino_boxes) == 0:
        print("Grounding DINO did not detect any objects.")
        return None

    h, w, _ = image_cv.shape
    dino_boxes_numpy = dino_boxes.cpu().numpy()
    dino_boxes_xyxy = dino_boxes_numpy * np.array([w, h, w, h])

    print(f"Found {len(dino_boxes)} potential objects. Segmenting with SAM...")
    masks, _, _ = sam_predictor.predict(
        point_coords=None,
        point_labels=None,
        box=dino_boxes_xyxy,
        multimask_output=False
    )
    return masks, image_cv

def visualize_masks(image, masks):
    """Simple visualization of masks on the image."""
    colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0]]
    if masks is not None:
        overlay = np.zeros_like(image, dtype=np.uint8)
        for i, mask in enumerate(masks):
            color = colors[i % len(colors)]
            overlay[mask] = color
        final_image = cv2.addWeighted(image, 0.5, overlay, 0.5, 0)
    else:
        final_image = image
    cv2_imshow(final_image)

-
if __name__ == "__main__":

    IMAGE_URL = ""
    TEXT_PROMPT = "a solar panel array"

    dino_model, sam_predictor = initialize_models()
    masks, image = run_segmentation(IMAGE_URL, TEXT_PROMPT, dino_model, sam_predictor)
    visualize_masks(image, masks)