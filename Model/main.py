# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YCoKimJzux_pUaK7WlmQ8fpB_xkhLsbZ
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import base64
from io import BytesIO
import cv2
import numpy as np
from PIL import Image
import torch
import torchvision.transforms as T
from groundingdino.util.inference import load_model, predict
from segment_anything import build_sam_vit_h, SamPredictor
import os


class SegmentationRequest(BaseModel):
    """
    Request model for the segmentation endpoint.
    image: Base64-encoded string of the input image.
    prompt: The text prompt for Grounding DINO.
    """
    image: str
    prompt: str


app = FastAPI(
    title="Zero-Shot Auditor API",
    description="An API for zero-shot image segmentation using Grounding DINO and SAM.",
    version="1.0.0",
)



DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")


DINO_CONFIG_PATH = "GroundingDINO_SwinT_OGC.py"
DINO_CHECKPOINT_PATH = "groundingdino_swint_ogc.pth"
SAM_CHECKPOINT_PATH = "sam_vit_h_4b8939.pth"

def initialize_models():
    """Initializes and loads the Grounding DINO and SAM models."""
    try:
        print("Initializing Grounding DINO model...")
        dino_model = load_model(DINO_CONFIG_PATH, DINO_CHECKPOINT_PATH)
        dino_model.to(DEVICE)
        dino_model.eval()

        print("Initializing SAM model...")
        sam = build_sam_vit_h(checkpoint=SAM_CHECKPOINT_PATH)
        sam.to(DEVICE)
        sam_predictor = SamPredictor(sam)

        return dino_model, sam_predictor
    except Exception as e:
        print(f"Error initializing models: {e}")

        raise RuntimeError(f"Could not load models: {e}")


dino_model, sam_predictor = initialize_models()



def zero_shot_segmentation(image_bytes, text_prompt, dino_model, sam_predictor):
    """
    Performs zero-shot segmentation using Grounding DINO for bounding boxes
    and SAM for segmentation masks.

    Args:
        image_bytes: The raw bytes of the image file.
        text_prompt: The object description text.
        dino_model: The loaded Grounding DINO model.
        sam_predictor: The loaded SAM predictor.

    Returns:
        A list of segmentation masks or None if no objects are detected.
    """

    image_pil = Image.open(BytesIO(image_bytes)).convert("RGB")
    image_cv = np.array(image_pil)


    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    image_tensor = transform(image_pil).to(DEVICE)


    sam_predictor.set_image(image_cv)
    print(f"Running Grounding DINO with prompt: '{text_prompt}'...")


    dino_boxes, _, _ = predict(
        model=dino_model,
        image=image_tensor,
        caption=text_prompt,
        box_threshold=0.3,
        text_threshold=0.25
    )

    if len(dino_boxes) == 0:
        print("Grounding DINO did not detect any objects for the given prompt.")
        return None


    h, w, _ = image_cv.shape
    dino_boxes_numpy = dino_boxes.cpu().numpy()
    dino_boxes_xyxy = dino_boxes_numpy * np.array([w, h, w, h])

    print(f"Grounding DINO detected {len(dino_boxes)} potential objects. Now using SAM for segmentation...")


    masks, _, _ = sam_predictor.predict(
        point_coords=None,
        point_labels=None,
        box=dino_boxes_xyxy,
        multimask_output=False
    )

    return masks



def visualize_results(image_bytes, masks):
    """
    Overlays the segmentation masks on the original image and returns a base64 string.

    Args:
        image_bytes: The raw bytes of the original image.
        masks: A list of segmentation masks from SAM.

    Returns:
        A base64-encoded string of the visualized image.
    """
    image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)

    colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0]]

    if masks is not None and len(masks) > 0:
        overlay = np.zeros_like(image, dtype=np.uint8)
        for i, mask in enumerate(masks):
            color = colors[i % len(colors)]
            overlay[mask] = color

        alpha = 0.5
        final_image = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)
    else:

        final_image = image

    _, buffer = cv2.imencode('.png', final_image)
    b64_image = base64.b64encode(buffer).decode('utf-8')
    return b64_image


@app.post("/segment")
async def segment_image(data: SegmentationRequest):
    """
    Processes an image and a text prompt to perform zero-shot segmentation.

    Args:
        data: The request body containing the image and prompt.

    Returns:
        A JSON response with the status, a message, and the base64-encoded segmented image.
    """
    try:
        image_bytes = base64.b64decode(data.image)
        masks = zero_shot_segmentation(image_bytes, data.prompt, dino_model, sam_predictor)
        output_image_b64 = visualize_results(image_bytes, masks)

        return {
            "status": "success",
            "message": f"Found {len(masks) if masks is not None else 0} objects.",
            "image": output_image_b64
        }
    except Exception as e:
        # Catch any unexpected errors and return a 500 server error
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")